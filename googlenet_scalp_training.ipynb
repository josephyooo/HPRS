{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25 | Learning Rate 0.001\n",
      "----------\n",
      "train Loss: 0.0125 Acc: 0.9525\n",
      "val Loss: 0.1512 Acc: 0.7324\n",
      "\n",
      "Epoch 2/25 | Learning Rate 0.001\n",
      "----------\n",
      "train Loss: 0.0195 Acc: 0.9410\n",
      "val Loss: 0.0582 Acc: 0.8521\n",
      "\n",
      "Epoch 3/25 | Learning Rate 0.001\n",
      "----------\n",
      "train Loss: 0.0124 Acc: 0.9481\n",
      "val Loss: 0.0194 Acc: 0.9366\n",
      "\n",
      "Epoch 4/25 | Learning Rate 0.001\n",
      "----------\n",
      "train Loss: 0.0084 Acc: 0.9710\n",
      "val Loss: 0.0519 Acc: 0.8873\n",
      "\n",
      "Epoch 5/25 | Learning Rate 0.001\n",
      "----------\n",
      "train Loss: 0.0149 Acc: 0.9569\n",
      "val Loss: 0.0705 Acc: 0.7887\n",
      "\n",
      "Epoch 6/25 | Learning Rate 0.001\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.9551\n",
      "val Loss: 0.0474 Acc: 0.8944\n",
      "\n",
      "Epoch 7/25 | Learning Rate 0.001\n",
      "----------\n",
      "train Loss: 0.0070 Acc: 0.9727\n",
      "val Loss: 0.0394 Acc: 0.9225\n",
      "\n",
      "Epoch 8/25 | Learning Rate 0.0001\n",
      "----------\n",
      "train Loss: 0.0053 Acc: 0.9806\n",
      "val Loss: 0.0395 Acc: 0.8873\n",
      "\n",
      "Epoch 9/25 | Learning Rate 0.0001\n",
      "----------\n",
      "train Loss: 0.0034 Acc: 0.9842\n",
      "val Loss: 0.0631 Acc: 0.8204\n",
      "\n",
      "Epoch 10/25 | Learning Rate 0.0001\n",
      "----------\n",
      "train Loss: 0.0038 Acc: 0.9850\n",
      "val Loss: 0.0136 Acc: 0.9542\n",
      "\n",
      "Epoch 11/25 | Learning Rate 0.0001\n",
      "----------\n",
      "train Loss: 0.0037 Acc: 0.9842\n",
      "val Loss: 0.0337 Acc: 0.8979\n",
      "\n",
      "Epoch 12/25 | Learning Rate 0.0001\n",
      "----------\n",
      "train Loss: 0.0017 Acc: 0.9947\n",
      "val Loss: 0.0215 Acc: 0.9296\n",
      "\n",
      "Epoch 13/25 | Learning Rate 0.0001\n",
      "----------\n",
      "train Loss: 0.0032 Acc: 0.9859\n",
      "val Loss: 0.0400 Acc: 0.8768\n",
      "\n",
      "Epoch 14/25 | Learning Rate 0.0001\n",
      "----------\n",
      "train Loss: 0.0018 Acc: 0.9921\n",
      "val Loss: 0.0269 Acc: 0.9261\n",
      "\n",
      "Epoch 15/25 | Learning Rate 1e-05\n",
      "----------\n",
      "train Loss: 0.0025 Acc: 0.9921\n",
      "val Loss: 0.0300 Acc: 0.9049\n",
      "\n",
      "Epoch 16/25 | Learning Rate 1e-05\n",
      "----------\n",
      "train Loss: 0.0018 Acc: 0.9912\n",
      "val Loss: 0.0263 Acc: 0.8944\n",
      "\n",
      "Epoch 17/25 | Learning Rate 1e-05\n",
      "----------\n",
      "train Loss: 0.0036 Acc: 0.9842\n",
      "val Loss: 0.0242 Acc: 0.9049\n",
      "\n",
      "Epoch 18/25 | Learning Rate 1e-05\n",
      "----------\n",
      "train Loss: 0.0044 Acc: 0.9798\n",
      "val Loss: 0.0271 Acc: 0.9155\n",
      "\n",
      "Epoch 19/25 | Learning Rate 1e-05\n",
      "----------\n",
      "train Loss: 0.0057 Acc: 0.9886\n",
      "val Loss: 0.0313 Acc: 0.8944\n",
      "\n",
      "Epoch 20/25 | Learning Rate 1e-05\n",
      "----------\n",
      "train Loss: 0.0020 Acc: 0.9894\n",
      "val Loss: 0.0343 Acc: 0.8873\n",
      "\n",
      "Epoch 21/25 | Learning Rate 1e-05\n",
      "----------\n",
      "train Loss: 0.0029 Acc: 0.9859\n",
      "val Loss: 0.0179 Acc: 0.9366\n",
      "\n",
      "Epoch 22/25 | Learning Rate 1.0000000000000002e-06\n",
      "----------\n",
      "train Loss: 0.0050 Acc: 0.9806\n",
      "val Loss: 0.0201 Acc: 0.9401\n",
      "\n",
      "Epoch 23/25 | Learning Rate 1.0000000000000002e-06\n",
      "----------\n",
      "train Loss: 0.0028 Acc: 0.9886\n",
      "val Loss: 0.0267 Acc: 0.9155\n",
      "\n",
      "Epoch 24/25 | Learning Rate 1.0000000000000002e-06\n",
      "----------\n",
      "train Loss: 0.0018 Acc: 0.9903\n",
      "val Loss: 0.0240 Acc: 0.9120\n",
      "\n",
      "Epoch 25/25 | Learning Rate 1.0000000000000002e-06\n",
      "----------\n",
      "train Loss: 0.0020 Acc: 0.9912\n",
      "val Loss: 0.0194 Acc: 0.9261\n",
      "\n",
      "Training complete in 3m 33s\n",
      "Best val Acc: 0.954225\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms.v2 as v2\n",
    "from torchvision.models import GoogLeNet_Weights\n",
    "\n",
    "from data_prep import DermNet, get_dataloaders\n",
    "from googlenet_scalp import GoogLeNet_Scalp\n",
    "from utils import train_model\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 64\n",
    "lr = 1e-3\n",
    "eps = 1e-4\n",
    "weight_decay = 1e-4\n",
    "weight_decay = 0\n",
    "step_size = 7\n",
    "gamma = 0.1\n",
    "num_epochs = 25\n",
    "\n",
    "# optimizations\n",
    "num_workers = 12\n",
    "pin_memory = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# get transform and augment\n",
    "transform = GoogLeNet_Weights.DEFAULT.transforms()\n",
    "## Reference: https://sebastianraschka.com/blog/2023/data-augmentation-pytorch.html\n",
    "augmenter = v2.RandAugment()\n",
    "\n",
    "# get dataset and data loaders\n",
    "dataset = DermNet(transform=transform)\n",
    "num_classes = len(dataset.classes)\n",
    "train_loader, val_loader = get_dataloaders(\n",
    "    dataset=dataset, transform=transform, batch_size=batch_size,\n",
    "    num_workers=num_workers, pin_memory=pin_memory\n",
    ")\n",
    "dataloaders = {'train': train_loader, 'val': val_loader}\n",
    "\n",
    "# setup model\n",
    "model_ft = GoogLeNet_Scalp(device=device, num_classes=num_classes)\n",
    "model_ft.load_state_dict(torch.load('weights/model_checkpoint.pt'))\n",
    "# model_ft.load_state_dict(torch.load('weights/googlenet_scalp_99.pt'))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), lr=lr, eps=eps, weight_decay=weight_decay)\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=step_size, gamma=gamma)\n",
    "\n",
    "model_ft = train_model(model_ft, dataloaders, criterion, optimizer_ft, lr_scheduler,\n",
    "                        num_epochs=num_epochs, device=device, augmenter=augmenter)\n",
    "# save model\n",
    "torch.save(model_ft.state_dict(), 'weights/model_checkpoint.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hair-deisease-cnn-project-sPLV6DJP-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
