{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25 | Learning Rate 1e-05\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jo-wubuntu/.cache/pypoetry/virtualenvs/hair-deisease-cnn-project-sPLV6DJP-py3.10/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0566 Acc: 0.9833\n",
      "val Loss: 0.1446 Acc: 0.9544\n",
      "\n",
      "Epoch 2/25 | Learning Rate 1e-05\n",
      "----------\n",
      "train Loss: 0.0359 Acc: 0.9912\n",
      "val Loss: 0.0864 Acc: 0.9579\n",
      "\n",
      "Epoch 3/25 | Learning Rate 1e-05\n",
      "----------\n",
      "train Loss: 0.0298 Acc: 0.9903\n",
      "val Loss: 0.0612 Acc: 0.9860\n",
      "\n",
      "Epoch 4/25 | Learning Rate 1e-05\n",
      "----------\n",
      "train Loss: 0.0333 Acc: 0.9868\n",
      "val Loss: 0.1970 Acc: 0.9474\n",
      "\n",
      "Epoch 5/25 | Learning Rate 1e-05\n",
      "----------\n",
      "train Loss: 0.0191 Acc: 0.9956\n",
      "val Loss: 0.1061 Acc: 0.9754\n",
      "\n",
      "Epoch 6/25 | Learning Rate 1e-05\n",
      "----------\n",
      "train Loss: 0.0250 Acc: 0.9938\n",
      "val Loss: 0.0278 Acc: 0.9895\n",
      "\n",
      "Epoch 7/25 | Learning Rate 1e-05\n",
      "----------\n",
      "train Loss: 0.0280 Acc: 0.9894\n",
      "val Loss: 0.1135 Acc: 0.9684\n",
      "\n",
      "Epoch 8/25 | Learning Rate 1.0000000000000002e-06\n",
      "----------\n",
      "train Loss: 0.0072 Acc: 0.9991\n",
      "val Loss: 0.0688 Acc: 0.9895\n",
      "\n",
      "Epoch 9/25 | Learning Rate 1.0000000000000002e-06\n",
      "----------\n",
      "train Loss: 0.0143 Acc: 0.9947\n",
      "val Loss: 0.1573 Acc: 0.9579\n",
      "\n",
      "Epoch 10/25 | Learning Rate 1.0000000000000002e-06\n",
      "----------\n",
      "train Loss: 0.0147 Acc: 0.9947\n",
      "val Loss: 0.0564 Acc: 0.9789\n",
      "\n",
      "Epoch 11/25 | Learning Rate 1.0000000000000002e-06\n",
      "----------\n",
      "train Loss: 0.0175 Acc: 0.9965\n",
      "val Loss: 0.0133 Acc: 0.9965\n",
      "\n",
      "Epoch 12/25 | Learning Rate 1.0000000000000002e-06\n",
      "----------\n",
      "train Loss: 0.0149 Acc: 0.9974\n",
      "val Loss: 0.2281 Acc: 0.9333\n",
      "\n",
      "Epoch 13/25 | Learning Rate 1.0000000000000002e-06\n",
      "----------\n",
      "train Loss: 0.0270 Acc: 0.9930\n",
      "val Loss: 0.0661 Acc: 0.9789\n",
      "\n",
      "Epoch 14/25 | Learning Rate 1.0000000000000002e-06\n",
      "----------\n",
      "train Loss: 0.0142 Acc: 0.9947\n",
      "val Loss: 0.1089 Acc: 0.9649\n",
      "\n",
      "Epoch 15/25 | Learning Rate 1.0000000000000002e-07\n",
      "----------\n",
      "train Loss: 0.0107 Acc: 0.9974\n",
      "val Loss: 0.2108 Acc: 0.9263\n",
      "\n",
      "Epoch 16/25 | Learning Rate 1.0000000000000002e-07\n",
      "----------\n",
      "train Loss: 0.0144 Acc: 0.9965\n",
      "val Loss: 0.0973 Acc: 0.9754\n",
      "\n",
      "Epoch 17/25 | Learning Rate 1.0000000000000002e-07\n",
      "----------\n",
      "train Loss: 0.0176 Acc: 0.9965\n",
      "val Loss: 0.0637 Acc: 0.9789\n",
      "\n",
      "Epoch 18/25 | Learning Rate 1.0000000000000002e-07\n",
      "----------\n",
      "train Loss: 0.0240 Acc: 0.9921\n",
      "val Loss: 0.1882 Acc: 0.9404\n",
      "\n",
      "Epoch 19/25 | Learning Rate 1.0000000000000002e-07\n",
      "----------\n",
      "train Loss: 0.0166 Acc: 0.9938\n",
      "val Loss: 0.0593 Acc: 0.9789\n",
      "\n",
      "Epoch 20/25 | Learning Rate 1.0000000000000002e-07\n",
      "----------\n",
      "train Loss: 0.0121 Acc: 0.9982\n",
      "val Loss: 0.0441 Acc: 0.9860\n",
      "\n",
      "Epoch 21/25 | Learning Rate 1.0000000000000002e-07\n",
      "----------\n",
      "train Loss: 0.0234 Acc: 0.9894\n",
      "val Loss: 0.2067 Acc: 0.9298\n",
      "\n",
      "Epoch 22/25 | Learning Rate 1.0000000000000004e-08\n",
      "----------\n",
      "train Loss: 0.0158 Acc: 0.9947\n",
      "val Loss: 0.0426 Acc: 0.9825\n",
      "\n",
      "Epoch 23/25 | Learning Rate 1.0000000000000004e-08\n",
      "----------\n",
      "train Loss: 0.0136 Acc: 0.9974\n",
      "val Loss: 0.2687 Acc: 0.9088\n",
      "\n",
      "Epoch 24/25 | Learning Rate 1.0000000000000004e-08\n",
      "----------\n",
      "train Loss: 0.0293 Acc: 0.9903\n",
      "val Loss: 0.0848 Acc: 0.9649\n",
      "\n",
      "Epoch 25/25 | Learning Rate 1.0000000000000004e-08\n",
      "----------\n",
      "train Loss: 0.0227 Acc: 0.9930\n",
      "val Loss: 0.0427 Acc: 0.9895\n",
      "\n",
      "Training complete in 3m 47s\n",
      "Best val Acc: 0.996491\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms.v2 as v2\n",
    "from torchvision.models import GoogLeNet_Weights\n",
    "\n",
    "from data_prep import DermNet, get_dataloaders\n",
    "from googlenet_scalp import GoogLeNet_Scalp\n",
    "from model_trainer import train_model\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 64\n",
    "lr = 1e-5\n",
    "eps = 1e-4\n",
    "weight_decay = 1e-4\n",
    "step_size = 7\n",
    "gamma = 0.1\n",
    "num_epochs = 25\n",
    "\n",
    "# optimizations\n",
    "num_workers = 12\n",
    "pin_memory = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# get transform and augment\n",
    "transform = GoogLeNet_Weights.DEFAULT.transforms()\n",
    "## Reference: https://sebastianraschka.com/blog/2023/data-augmentation-pytorch.html\n",
    "augmenter = v2.RandAugment()\n",
    "\n",
    "# get dataset and data loaders\n",
    "dataset = DermNet(transform=transform)\n",
    "num_classes = len(dataset.classes)\n",
    "train_loader, val_loader = get_dataloaders(\n",
    "    dataset=dataset, transform=transform, batch_size=batch_size,\n",
    "    num_workers=num_workers, pin_memory=pin_memory\n",
    ")\n",
    "dataloaders = {'train': train_loader, 'val': val_loader}\n",
    "\n",
    "# setup model\n",
    "model_ft = GoogLeNet_Scalp(device=device, num_classes=num_classes)\n",
    "# model_ft.load_state_dict(torch.load('weights/model_checkpoint.pt'))\n",
    "model_ft.load_state_dict(torch.load('weights/googlenet_scalp_99.pt'))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), lr=lr, eps=eps, weight_decay=weight_decay)\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=step_size, gamma=gamma)\n",
    "\n",
    "model_ft = train_model(model_ft, dataloaders, criterion, optimizer_ft, lr_scheduler,\n",
    "                        num_epochs=num_epochs, device=device, augmenter=augmenter)\n",
    "# save model\n",
    "torch.save(model_ft.state_dict(), 'weights/model_checkpoint.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hair-deisease-cnn-project-sPLV6DJP-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
