{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25 | Learning Rate 0.0001\n",
      "----------\n",
      "train Loss: 0.0614 Acc: 0.9797\n",
      "val Loss: 0.4648 Acc: 0.8566\n",
      "\n",
      "Epoch 2/25 | Learning Rate 0.0001\n",
      "----------\n",
      "train Loss: 0.0575 Acc: 0.9825\n",
      "val Loss: 0.0355 Acc: 0.9890\n",
      "\n",
      "Epoch 3/25 | Learning Rate 0.0001\n",
      "----------\n",
      "train Loss: 0.0347 Acc: 0.9899\n",
      "val Loss: 0.0745 Acc: 0.9779\n",
      "\n",
      "Epoch 4/25 | Learning Rate 0.0001\n",
      "----------\n",
      "train Loss: 0.0274 Acc: 0.9899\n",
      "val Loss: 0.0452 Acc: 0.9926\n",
      "\n",
      "Epoch 5/25 | Learning Rate 0.0001\n",
      "----------\n",
      "train Loss: 0.0416 Acc: 0.9871\n",
      "val Loss: 0.2762 Acc: 0.9449\n",
      "\n",
      "Epoch 6/25 | Learning Rate 0.0001\n",
      "----------\n",
      "train Loss: 0.0207 Acc: 0.9954\n",
      "val Loss: 0.0368 Acc: 0.9890\n",
      "\n",
      "Epoch 7/25 | Learning Rate 0.0001\n",
      "----------\n",
      "train Loss: 0.0428 Acc: 0.9853\n",
      "val Loss: 0.1189 Acc: 0.9559\n",
      "\n",
      "Epoch 8/25 | Learning Rate 1e-05\n",
      "----------\n",
      "train Loss: 0.0225 Acc: 0.9908\n",
      "val Loss: 0.0576 Acc: 0.9853\n",
      "\n",
      "Epoch 9/25 | Learning Rate 1e-05\n",
      "----------\n",
      "train Loss: 0.0224 Acc: 0.9926\n",
      "val Loss: 0.2341 Acc: 0.9118\n",
      "\n",
      "Epoch 10/25 | Learning Rate 1e-05\n",
      "----------\n",
      "train Loss: 0.0217 Acc: 0.9935\n",
      "val Loss: 0.2329 Acc: 0.9191\n",
      "\n",
      "Epoch 11/25 | Learning Rate 1e-05\n",
      "----------\n",
      "train Loss: 0.0139 Acc: 0.9972\n",
      "val Loss: 0.0501 Acc: 0.9890\n",
      "\n",
      "Epoch 12/25 | Learning Rate 1e-05\n",
      "----------\n",
      "train Loss: 0.0217 Acc: 0.9935\n",
      "val Loss: 0.3377 Acc: 0.8897\n",
      "\n",
      "Epoch 13/25 | Learning Rate 1e-05\n",
      "----------\n",
      "train Loss: 0.0172 Acc: 0.9945\n",
      "val Loss: 0.1321 Acc: 0.9412\n",
      "\n",
      "Epoch 14/25 | Learning Rate 1e-05\n",
      "----------\n",
      "train Loss: 0.0198 Acc: 0.9972\n",
      "val Loss: 0.2300 Acc: 0.9301\n",
      "\n",
      "Epoch 15/25 | Learning Rate 1.0000000000000002e-06\n",
      "----------\n",
      "train Loss: 0.0294 Acc: 0.9926\n",
      "val Loss: 0.1763 Acc: 0.9338\n",
      "\n",
      "Epoch 16/25 | Learning Rate 1.0000000000000002e-06\n",
      "----------\n",
      "train Loss: 0.0180 Acc: 0.9945\n",
      "val Loss: 0.0742 Acc: 0.9706\n",
      "\n",
      "Epoch 17/25 | Learning Rate 1.0000000000000002e-06\n",
      "----------\n",
      "train Loss: 0.0154 Acc: 0.9963\n",
      "val Loss: 0.2771 Acc: 0.9044\n",
      "\n",
      "Epoch 18/25 | Learning Rate 1.0000000000000002e-06\n",
      "----------\n",
      "train Loss: 0.0131 Acc: 0.9963\n",
      "val Loss: 0.2212 Acc: 0.9154\n",
      "\n",
      "Epoch 19/25 | Learning Rate 1.0000000000000002e-06\n",
      "----------\n",
      "train Loss: 0.0232 Acc: 0.9926\n",
      "val Loss: 0.1422 Acc: 0.9596\n",
      "\n",
      "Epoch 20/25 | Learning Rate 1.0000000000000002e-06\n",
      "----------\n",
      "train Loss: 0.0273 Acc: 0.9926\n",
      "val Loss: 0.0504 Acc: 0.9926\n",
      "\n",
      "Epoch 21/25 | Learning Rate 1.0000000000000002e-06\n",
      "----------\n",
      "train Loss: 0.0264 Acc: 0.9926\n",
      "val Loss: 0.1765 Acc: 0.9522\n",
      "\n",
      "Epoch 22/25 | Learning Rate 1.0000000000000002e-07\n",
      "----------\n",
      "train Loss: 0.0129 Acc: 0.9982\n",
      "val Loss: 0.0507 Acc: 0.9816\n",
      "\n",
      "Epoch 23/25 | Learning Rate 1.0000000000000002e-07\n",
      "----------\n",
      "train Loss: 0.0096 Acc: 0.9982\n",
      "val Loss: 0.0109 Acc: 1.0000\n",
      "\n",
      "Epoch 24/25 | Learning Rate 1.0000000000000002e-07\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.9982\n",
      "val Loss: 0.1147 Acc: 0.9669\n",
      "\n",
      "Epoch 25/25 | Learning Rate 1.0000000000000002e-07\n",
      "----------\n",
      "train Loss: 0.0105 Acc: 0.9963\n",
      "val Loss: 0.0687 Acc: 0.9743\n",
      "\n",
      "Training complete in 3m 19s\n",
      "Best val Acc: 1.000000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms.v2 as v2\n",
    "from torchvision.models import GoogLeNet_Weights\n",
    "\n",
    "from data_prep import DermNet, get_dataloaders\n",
    "from googlenet_scalp import GoogLeNet_Scalp\n",
    "from model_trainer import train_model\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 64\n",
    "lr = 1e-4\n",
    "eps = 1e-4\n",
    "weight_decay = 1e-4\n",
    "step_size = 7\n",
    "gamma = 0.1\n",
    "num_epochs = 25\n",
    "\n",
    "# optimizations\n",
    "num_workers = 12\n",
    "pin_memory = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# get transform and augment\n",
    "transform = GoogLeNet_Weights.DEFAULT.transforms()\n",
    "## Reference: https://sebastianraschka.com/blog/2023/data-augmentation-pytorch.html\n",
    "augmenter = v2.RandAugment()\n",
    "\n",
    "# get dataset and data loaders\n",
    "dataset = DermNet(transform=transform)\n",
    "num_classes = len(dataset.classes)\n",
    "train_loader, val_loader = get_dataloaders(\n",
    "    dataset=dataset, transform=transform, batch_size=batch_size,\n",
    "    num_workers=num_workers, pin_memory=pin_memory\n",
    ")\n",
    "dataloaders = {'train': train_loader, 'val': val_loader}\n",
    "\n",
    "# setup model\n",
    "model_ft = GoogLeNet_Scalp(device=device, num_classes=num_classes)\n",
    "# model_ft.load_state_dict(torch.load('weights/model_checkpoint.pt'))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), lr=lr, eps=eps, weight_decay=weight_decay)\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=step_size, gamma=gamma)\n",
    "\n",
    "model_ft = train_model(model_ft, dataloaders, criterion, optimizer_ft, lr_scheduler,\n",
    "                        num_epochs=num_epochs, device=device, augmenter=augmenter)\n",
    "# save model\n",
    "torch.save(model_ft.state_dict(), 'weights/model_checkpoint.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hair-deisease-cnn-project-sPLV6DJP-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
